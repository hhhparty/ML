# 奇异值分解 SVD

> 这是一篇学习笔记，内容来自：数学中国 https://mp.weixin.qq.com/s/fTvAT9Kr0Dfx9PqDpJ9_6w

奇异值分解是一个很有物理意义的方法。它可以将一个比较复杂的矩阵用更小、更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要特征。

机器学习中，很多应用与奇异值分解有关，比如PCA、数据压缩算法、搜索引擎语义层检索LSI(latent semantic indexing)等等。

## 特征值与奇异值

特征值分解和奇异值分解，两者又很紧密的关系，他们的目的都是一样，就是提取一个矩阵中重要的特征。

### 特征值

定义：一个向量v是方阵A的特征向量，将一定可以表示称下面的形式：$Av = \lambda v$，此时，$\lambda$称为特征向量v对应的特征值。

一个矩阵的一组特征向量是一组正交向量。

特征值分解是讲一个矩阵分解成下面的形式：

$A = Q\sum Q^{-1}$

其中Q是这个矩阵A的特征向量组成的矩阵，$\sum$是一个对角矩阵，每一个对角线上的元素就是一个特征值。这里要注意：
- 要明确，一个矩阵就是一个线性变换，因为一个矩阵乘以一个向量后得到一个向量，其实相当于将这个向量进行了线性变换。

例如，对角线矩阵对向量v的变换，即是将向量变换为$(\lambda_1 v_1,\lambda_2 v_2,...,\lambda_n v_n)$

更一般地，当矩阵是高维时，这个矩阵就是高维空间的一个线性变换，这个变换同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（线性变换）。

总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。 

### 奇异值

特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵。

比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？ 奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法：

$A = U\sum V^T$

假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），$V^T$(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量），从图片来反映几个相乘的矩阵的大小可得下面的图片。

那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置$A^T$，将会得到一个方阵，我们用这个方阵求特征值可以得到：

$(A^TA)v_i = \lambda_i v_i$

这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：

$ \sigma_i = \sqrt{\lambda_i}$



