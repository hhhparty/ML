{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用urllib获取www资源\n",
    "\n",
    "目标：学习使用urllib编写简单爬虫程序，获取webpage资源\n",
    "\n",
    "\n",
    "1.使用urlopen()实现最简单的url访问，获取所指页面内容。\n",
    "\n",
    "2.使用urlretireve()将页面内容存为临时文件，并获取response头\n",
    "\n",
    "3.定制request对象，使爬虫更像浏览器\n",
    "    默认情况下，urllib发出的请求头如下所示（用wireshark工具截获可知）：\n",
    "        GET / HTTP/1.1\n",
    "        Accept-Encoding: identity\n",
    "        Host: 10.10.10.135\n",
    "        User-Agent: Python-urllib/3.6\n",
    "        Connection: close\n",
    "4.利用GET方法，向百度服务器发送查询请求，并获得查询结果 \n",
    "\n",
    "5.利用POST方法，向http://10.10.10.135/WebGoat/ 提交用户名和密码\n",
    "\n",
    "6.利用urllib.error处理异常,两个常用异常类：urllib.error.URLError和HTTPError\n",
    "\n",
    "7.认识response类的方法：info(),geturl()\n",
    "\n",
    "8.一个较为完整case，从百度贴吧下载多页话题\n",
    "\n",
    "\n",
    "   \n",
    "9.通过HTTP basic authentication\n",
    "    登陆网页前遇到的要求输入用户名和密码的程序，通常称为身份认证程序。\n",
    "    HTTP认证可以保护一个作用域（称为一个realm）内的资源不受非法访问。\n",
    "    HTTP规范中定义了两种认证模式：basic auth和digest auth\n",
    "    认证的基本过程是：1.客户请求访问网页；2.服务器端返回401错误，要求认证；\n",
    "    3.客户端重新提交请求并附以认证信息，这部分信息将被编码；\n",
    "    4.服务器检查信息，通过则给以正常服务页面；否则返回401错误。\n",
    "    \n",
    "   第一次服务器返回401错误时，会返回headers字典信息，其中会包含如下信息：\n",
    "    WWW-Authenticate: Basic realm=\"cPanel\"\n",
    "    我们假定已知用户名和密码，之后利用一定的编码格式将realm名、用户名、密码等信息\n",
    "    编码后就可以传递给服务器，认证就可通过。\n",
    "    编码格式是base-64\n",
    "\n",
    "10.Opener与Handler\n",
    "    urllib.request中含有多个Handler和Opener类\n",
    "    BaseHandler 、HTTPRedirectHandler、HTTPCookieProcessor、ProxyHandler、HTTPBasicAuthHandler、ProxyBasicAuthHandler、AbstractBasicAuthHandler、AbstractDigestAuthHandler、HTTPDigestAuthHandler、ProxyDigestAuthHandler、HTTPHandler、HTTPSHandler、 FileHandler、DataHandler、CacheFTPHandler、UnknownHandler\n",
    "OpenerDirector;\n",
    "\n",
    "   当我们抓取URL时，都会使用一个opener，urllib中的opener都是urllib.request.OpenerDirector类的实例。通常我们使用默认的opener，通过urlopen。我们也可以自己定义opener。\n",
    "   opener使用handler，重活都是由handler完成的。\n",
    "   每个handler知道如何打开一个特殊URL schema的URLs，例如http、ftp等，或者控制一种特殊的http访问，例如http重定向或http cookies.\n",
    "   自定义opener需要先初始化一个OpenerDirector，并调用.add_handler(some_handler_instance).还可以使用build_opener, 这是一个使用调用单一函数调用多个handlers生成opener实例的方法。\n",
    "   install_opener可以用于生成一个opener对象，形成全局默认的opener，这使你再次使用urlopen时，不再使用系统原opener，而使用你定义的opener。\n",
    "   不准备替换全局默认的opener时，可以使用opener实例中的open方法，访问url。\n",
    "   \n",
    "   可以先看一下urllib.request.urlopen源码\n",
    "   常见过程：1.myhandler = urllib.request.\n",
    "\n",
    "11.使用代理\n",
    "    \n",
    "   使用代理是对抗反爬虫机制的常用做法。\n",
    "   很多网站会检测某一段时间某个外来IP地址对服务器的访问次数等信息。\n",
    "   如果访问次数或方式不符合安全策略，就会禁止该外来IP对服务器的访问。\n",
    "   所以，爬虫设计者可以用一些代理服务器，使自己真实IP地址被隐藏，免于被禁止。\n",
    "   urllib中使用ProxyHandler来设置代理服务器的使用。\n",
    "    \n",
    "   网络上通常有两类代理：免费代理、收费代理。\n",
    "   免费代理可以通过百度/google搜索，或从以下网站查找：\n",
    "       西刺免费代理IP、快代理免费代理、Proxy360代理、全网代理IP...\n",
    "       免费开放代理一般会有很多人都在使用，而且代理有寿命短，速度慢，匿名度不高，HTTP/HTTPS支持不稳定等缺点（免费没好货）\n",
    "       专业爬虫工程师或爬虫公司会使用高品质的私密代理，这些代理通常需要找专门的代理供应商购买，再通过用户名/密码授权使用（舍不得孩子套不到狼）\n",
    "   可以组织一个代理列表，在一定时间策略下，随机使用，免于被server禁止访问。\n",
    "   \n",
    "   \n",
    "12.设置time-out\n",
    "13.使用HEAD方法，请求服务器\n",
    "\n",
    "14.urllib加载ajax信息\n",
    "   AJAX = Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）。\n",
    "   AJAX 最大的优点是在不重新加载整个页面的情况下，可以与服务器交换数据并更新部分网页内容。\n",
    "   AJAX 不需要任何浏览器插件，但需要用户允许JavaScript在浏览器上执行。\n",
    "    \n",
    "   这里以 https://movie.douban.com/tag/#/ 为例\n",
    "   先使用抓包工具查看一下这个页面，通过测试可以发现每次点击“更多”会增加一个响应    \n",
    "   https://movie.douban.com/j/new_search_subjects?sort=U&range=0,10&tags=&start=40\n",
    "   将其直接在浏览器中打开，可以看到它以json格式记录了新加载的电影信息。\n",
    "   找到这个文件后，就可开始尝试了。\n",
    "15.使用cookie访问网站\n",
    "    Cookie，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密)。 \n",
    "   比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容，登陆前与登陆后是不同的，或者不允许的。 \n",
    "   使用Cookie和使用代理IP一样，也需要创建一个自己的opener。在HTTP包中，提供了cookiejar模块，用于提供对Cookie的支持。\n",
    "   方法1：urllib通过已登录的cookie值，以登录用户身份访问网页。\n",
    "    首先用浏览器登录，获取登陆后的cookie，通常这个cookie会非常长。\n",
    "    我们以访问http://www.renren.com/968196747/profile 这个登录后链接为例 如果成功会得到相关内容.\n",
    "    将cookkie值作为字符串加载在headers里。\n",
    "   方法2：使用cookie处理库自动抓取登录cookie\n",
    "   http.cookieJar\n",
    "    \n",
    "16.访问https站点\n",
    "    需要CA证书才能访问,证书是用于加密连接和身份认证的数字凭据，通常由公信机构发放。\n",
    "    尝试访问http://www.baidu.com 与https://www.baidu.com， 观察它们的不同\n",
    "    百度访问https时会有跳转。\n",
    "    再尝试访问12306网站 'http://www.12306.cn/mormhweb/' 与 https://www.12306.cn/mormhweb/ 的不同\n",
    "    可以看到访问 https://www.12306.cn/mormhweb/ 时会报出错误：CertificateError: hostname 'www.12306.cn' doesn't match either of 'webssl.chinanetcenter.com'\n",
    "    ssl库ssl.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1.使用urlopen()实现最简单的url访问\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "\n",
    "url = 'http://10.10.10.135/'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    print(response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"2.使用urlretireve()将页面内容存为临时文件，并获取response头\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "\n",
    "url = 'http://10.10.10.135/'\n",
    "localfile, headers = urllib.request.urlretrieve(url)\n",
    "print(localfile)\n",
    "print()\n",
    "print(headers)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3.定制request对象，使爬虫更像浏览器\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "\n",
    "url = 'http://10.10.10.135/'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"4.使用GET方法，向百度服务器发送查询请求\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "querystr = {'wd':'北航'}\n",
    "querystr_encode = urllib.parse.urlencode(querystr)\n",
    "print(querystr_encode)\n",
    "#https://www.baidu.com/s?wd=%E5%8C%97%E8%88%AA\n",
    "url = 'http://www.baidu.com/s?' + querystr_encode\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n",
    "    print(response.headers)\n",
    "    html = response.read()\n",
    "    print(html.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5.利用POST方法，向http://10.10.10.135/WebGoat/ 提交用户名和密码\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = 'http://10.10.10.135/dvwa/login.php'\n",
    "cookie = 'PHPSESSID=898c1rsum58475qh3nros002n7; path=/'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "         'Cookie':cookie,\n",
    "          }\n",
    "authstr = {'username':'admin',\n",
    "           'password':'admin',\n",
    "           'Login':'Login',}\n",
    "data = urllib.parse.urlencode(authstr).encode('utf-8')\n",
    "\n",
    "request = urllib.request.Request(url,data=data,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n",
    "    print(response.headers)\n",
    "    cookie1 = response.headers['Set-Cookie']\n",
    "\n",
    "url = 'http://10.10.10.135/dvwa/index.php'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          'Cookie':cookie+';'+cookie1,\n",
    "          }\n",
    "print(headers)\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 设置time-out\"\"\"\n",
    "import socket\n",
    "import urllib.request\n",
    "# timeout in seconds\n",
    "timeout = 3\n",
    "socket.setdefaulttimeout(timeout)\n",
    "# this call to urllib.request.urlopen now uses the default timeout\n",
    "# we have set in the socket module\n",
    "req = urllib.request.Request('http://www.python.org/')\n",
    "a = urllib.request.urlopen(req).read()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"6. 使用urlllib.error处理异常\n",
    "URLError继承自OSError，是urllib的异常的基础类\n",
    "HTTPError是验证HTTP response实例的一个异常类。\n",
    "\n",
    "HTTP protocol errors是有效的response，有状态码、headers、body。\n",
    "\n",
    "一个成熟的程序需要管理所有输出，不仅有希望见到的输出，还要有意料之外的异常。\n",
    "logging的使用可以参考https://docs.python.org/3.5/howto/logging.html\n",
    "\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    filename='C:\\\\Users\\\\leo\\Documents\\\\crawlerslesson1_crawler.log',\n",
    "                    level=logging.DEBUG)\n",
    "try: \n",
    "    #url = 'http://www.baidu11.com'\n",
    "    url = 'http://10.10.10.135/WebGoat/attack'\n",
    "    headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "    request = urllib.request.Request(url,headers=headers)\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        print(response.status)\n",
    "        print(response.read().decode('utf-8'))\n",
    "\n",
    "except urllib.error.HTTPError as e:\n",
    "    import http.server\n",
    "    #print(http.server.BaseHTTPRequestHandler.responses[e.code])\n",
    "    logging.error('HTTPError code: %s and Messages: %s'% (str(e.code),http.server.BaseHTTPRequestHandler.responses[e.code]))\n",
    "    logging.info('HTTPError headers: ' + str(e.headers))\n",
    "    logging.info(e.read().decode('utf-8'))\n",
    "    print('不好意思，服务器卡壳儿了，请稍后重试。')\n",
    "except urllib.error.URLError as e:\n",
    "    logging.error(e.reason)\n",
    "    print('不好意思，服务器卡壳儿了，请稍后重试。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"7. 使用response的geturl和info方法来验证访问是否相符\n",
    "geturl - this returns the real URL of the page fetched. \n",
    "This is useful because urlopen (or the opener object used) may have followed a redirect. \n",
    "The URL of the page fetched may not be the same as the URL requested.\n",
    "\n",
    "info - this returns a dictionary-like object that describes the page fetched,\n",
    "particularly the headers sent by the server.\n",
    "It is currently an http.client.HTTPMessage instance.\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = 'http://www.baidu.com'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    #print(type(response))\n",
    "    print(response.info())\n",
    "    print(response.geturl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"8. A case of crawler is used to fetch the content of baidu's tieba url, in according to user's input keywords.\n",
    "\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "def loadPage(url):\n",
    "    \"\"\"\n",
    "        Function: Fetching url and accessing the webpage content.\n",
    "        url: the wanted webpage url.\n",
    "    \"\"\"\n",
    "    headers = {'Accept': 'text/html','User-Agent':'Mozilla/5.0',}\n",
    "    print('To send http request to %s' % url)\n",
    "    request = urllib.request.Request(url,headers=headers)\n",
    "\n",
    "    return  urllib.request.urlopen(request).read()\n",
    "\n",
    "def writePage(html,filename):\n",
    "    \"\"\"\n",
    "        Fuction: To write the content of html into a local file.\n",
    "        html: The response content.\n",
    "        filename: the local filename to be used stored the response.\n",
    "    \"\"\"\n",
    "    print('To write html into a local file %s ...' % filename)\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(str(html))\n",
    "    print('Work done.')\n",
    "    print('-'*10)\n",
    "\n",
    "def tiebaCrawler(url,beginpPage,endPage,keyword):\n",
    "    \"\"\"\n",
    "        Function: The scheduler of tieba crawler, is used to access every wanted url in turns.\n",
    "        url: the url of baidu's tieba webpage\n",
    "        beginPage: initial page\n",
    "        endPage: end page\n",
    "        keyword: the wanted keyword \n",
    "    \"\"\"\n",
    "    filename = keyword + '_tieba.html'\n",
    "    for page in range(beginpPage,endPage+1):\n",
    "        pn = (page - 1) * 50\n",
    "        queryurl = url + '&pn=' + str(pn)\n",
    "        #print(queryurl)\n",
    "        \n",
    "        writePage(loadPage(queryurl),filename)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    kw = input('Pl input the wanted tieba\\'s name:' )\n",
    "    beginPage = int(input('The beginning page number:'))\n",
    "    endPage = int(input('The ending page number:'))\n",
    "    url = 'http://tieba.baidu.com/f?'\n",
    "    key = urllib.parse.urlencode({'kw':kw})\n",
    "    queryurl = url+ key\n",
    "    tiebaCrawler(url,beginPage,endPage,kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"9. 不使用HTTPPasswordMgrWithDefaultRealm类的验证An case of passing basic authentication with urllib\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import logging\n",
    "\n",
    "url = 'http://10.10.10.135/WebGoat/attack'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "\n",
    "def passBasicAuth(realm):\n",
    "\n",
    "    import base64\n",
    "    username = 'webgoat'\n",
    "    password = 'webgoat'\n",
    "    bstr = username+':'+password\n",
    "    schemastr , realmname = realm.split('=')\n",
    "    if schemastr.lower().find('basic') >= 0:\n",
    "        schema ='Basic'\n",
    "    else:\n",
    "        print('The authentication schema isn\\'t basic, programe exit.')\n",
    "        exit(-1)\n",
    "        \n",
    "    base64str = base64.b64encode(bstr.encode('utf-8'))\n",
    "    authheader = 'Basic %s' % base64str.decode('utf-8')\n",
    "    \n",
    "    request.add_header('Authorization',authheader)\n",
    "    print(request.headers)\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        print(response.status)\n",
    "        print(response.read().decode('utf-8'))\n",
    "    \n",
    "     \n",
    "    \n",
    "try:\n",
    "    #url = 'http://www.baidu11.com'\n",
    "    \n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        print(response.status)\n",
    "        print(response.info())\n",
    "        #print(response.read().decode('utf-8'))\n",
    "except urllib.error.URLError as e:\n",
    "    if  hasattr(e,'code'):\n",
    "        \n",
    "        print(e.code)\n",
    "        print(e.info())\n",
    "        if e.code == 401:\n",
    "            passBasicAuth(e.headers['WWW-Authenticate'])            \n",
    "    elif hasattr(e,'reason'):\n",
    "        print(e.reason)\n",
    "    else:\n",
    "        print('unkown error.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"9. HTTPPasswordMgrWithDefaultRealm 的验证An case of passing basic authentication with urllib\n",
    "\n",
    "Worked!\n",
    "\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "\n",
    "url = 'http://10.10.10.135/WebGoat/attack'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "username = 'webgoat'\n",
    "password = 'webgoat'\n",
    "\n",
    "passman = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "# this creates a password manager\n",
    "passman.add_password(None, url, username, password)\n",
    "# because we have put None at the start it will always\n",
    "# use this username/password combination for  urls\n",
    "# for which `theurl` is a super-url\n",
    "\n",
    "authhandler = urllib.request.HTTPBasicAuthHandler(passman)\n",
    "# create the AuthHandler\n",
    "\n",
    "opener = urllib.request.build_opener(authhandler)\n",
    "\n",
    "urllib.request.install_opener(opener)\n",
    "# All calls to urllib2.urlopen will now use our handler\n",
    "# Make sure not to include the protocol in with the URL, or\n",
    "# HTTPPasswordMgrWithDefaultRealm will be very confused.\n",
    "# You must (of course) use it when fetching the page though.\n",
    "\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n",
    "    print(response.read().decode('utf-8'))\n",
    "# authentication is now handled automatically for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"10. 自定义opener\"\"\"\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "# demo 1\n",
    "httphandler = urllib.request.HTTPHandler()\n",
    "opener = urllib.request.build_opener(httphandler)\n",
    "request = urllib.request.Request('http://www.baidu.com/')\n",
    "response = opener.open(request)\n",
    "print(response.read().decode('utf-8'))\n",
    "\n",
    "# demo 2\n",
    "# 在开发中，如果需要了解HTTPHandler的调试信息，可以使用下列语句,无需输出语句。\n",
    "httphandler = urllib.request.HTTPHandler(debuglevel=1)\n",
    "opener = urllib.request.build_opener(httphandler)\n",
    "request = urllib.request.Request('http://www.baidu.com/')\n",
    "response = opener.open(request)\n",
    "\n",
    "\n",
    "# demo3  使用install_opener方法使自定义的opener成为全局默认opener\n",
    "\n",
    "request = urllib.request.Request('http://www.baidu.com/')\n",
    "\n",
    "httphandler = urllib.request.HTTPHandler()\n",
    "opener = urllib.request.build_opener(httphandler)\n",
    "urllib.request.install_opener(opener)\n",
    " \n",
    "a = urllib.request.urlopen(request).read().decode(\"utf8\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"11. 使用代理\"\"\"\n",
    "\n",
    "#demo1 使用ProxyHandler通过指定的一个免费代理访问目标网站\n",
    "import urllib.request\n",
    "\n",
    "request = urllib.request.Request('http://www.baidu.com/')\n",
    "proxy_support = urllib.request.ProxyHandler({'http': '210.1.58.212:8080'})\n",
    " \n",
    "opener = urllib.request.build_opener(proxy_support) \n",
    "response = opener.open(request)\n",
    "print(response.read().decode('utf-8'))\n",
    "\n",
    "\n",
    "#demo2 使用用户名、密码验证的代理爬取网站\n",
    "\n",
    "proxySwitcher = True\n",
    "httpWithProxyHandler = urllib.request.ProxyHandler({'http':'106.185.26.199:25'})\n",
    "httpWithoutProxyHandler = urllib.request.ProxyHandler({})\n",
    "\n",
    "if proxySwitcher:\n",
    "    opener = urllib.request.build_opener(httpWithProxyHandler)\n",
    "else:\n",
    "    opener = urllib.request.build_opener(httpWithoutProxyHandler)\n",
    "\n",
    "urllib.request.install_opener(opener)#使自定义opener成为全局默认opener\n",
    "request = urllib.request.Request('http://www.google.com/')\n",
    "response = urllib.request.urlopen(request)\n",
    "print(response.read().decode('utf-8'))\n",
    "\n",
    "#demo3 使用有身份认证的代理\n",
    "username = 'leo'\n",
    "password = 'leo'\n",
    "proxydict = {'http':'106.185.26.199:25'}\n",
    "proxydict['http'] = username+':'+password+'@'+proxydict['http']\n",
    "httpWithProxyHandler = urllib.request.ProxyHandler(proxydict)\n",
    "opener = urllib.request.build_opener(httpWithProxyHandler)\n",
    "request = urllib.request.Request('http://www.google.com/')\n",
    "response = opener.open(request)\n",
    "print(response.read().decode('utf-8'))\n",
    "\n",
    "#demo4 使用urllib推荐做法改进上述过程\n",
    "username = 'leo'\n",
    "password = 'leo'\n",
    "proxyserver = {'106.185.26.199:25'}\n",
    "\n",
    "#    1.构建一个密码管理对象，用来保存需要处理的用户名和密码；\n",
    "passwordMgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "#    2.添加用户信息，第一个参数realm是与远程服务器相关的域的信息，默认为None，可通过response头查看\n",
    "#      后面3个参数分别为代理服务器、用户名、密码；\n",
    "passwordMgr.add_password(None,proxyserver,username,password)\n",
    "#    3.构建一个代理基础用户名/密码验证的Handler对象，参数为密码管理对象；\n",
    "proxyauth_handler = urllib.request.ProxyBasicAuthHandler(passwordMgr)\n",
    "#    4.通过build_opener()定义opener对象\n",
    "opener = urllib.request.build_opener(proxyauth_handler)\n",
    "#    5.构造请求request\n",
    "request = urllib.request.Request('http://www.google.com')\n",
    "#    6.使用自定义opener发送请求\n",
    "response = opener.open(request)\n",
    "#    7.打印响应内容\n",
    "print(response.read().decode('utf-8'))\n",
    "\n",
    "\n",
    "#demo5 使用从http://www.goubanjia.com/，wwwkuaidaili.com/dps 获取的代理列表\n",
    "#可以使用快代理在线测试代理可行性\n",
    "import random\n",
    "proxylist = {\"https\":\"180.210.205.199:8888\",\n",
    "            \"http\":\"185.22.174.65:10010\",\n",
    "            \"http\":\"54.153.171.50:3128\",\n",
    "            \"http\":\"54.79.47.128:8080\",\n",
    "            \"https\":\"195.235.204.60:3128\",\n",
    "            \"http\":\"203.174.90.201:8080\",\n",
    "            \"http\":\"67.63.33.7:80\",\n",
    "            \"http\":\"150.138.220.247:80\",\n",
    "            \"http\":\"94.16.117.29:3128\",\n",
    "            \"http\":\"210.1.58.212:8080\",\n",
    "            \"http\":\"125.39.9.35:9000\",\n",
    "            \"http\":\"94.242.55.108:10010\",\n",
    "            \"http\":\"74.82.50.155:3128\",\n",
    "            \"https\":\"119.28.195.93:8888\",\n",
    "            \"http\":\"151.106.12.251:1080\",\n",
    "            \"https\":\"51.254.50.239:3128\",\n",
    "            \"http\":\"140.227.60.114:3128\",\n",
    "            \"http\":\"165.227.45.213:8080\",\n",
    "            \"http\":\"212.77.138.161:41258\",\n",
    "            \"http\":\"37.61.224.107:8195\",\n",
    "            }\n",
    "#    随机选择代理\n",
    "proxy = random.choice(proxylist)\n",
    "#    使用代理构建处理对象\n",
    "httpProxyHandler = urllib.request.ProxyHandler(proxy)\n",
    "opener = urllib.request.build_opener(httpProxyHandler)\n",
    "request = urllib.request.Request('http://www.google.com')\n",
    "response = opener.open(request)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"抓取ajax页面\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "url = 'https://movie.douban.com/j/new_search_subjects?'\n",
    "movietype = '动作'\n",
    "params = {'sort':'U','range':'0,10','tags':'','start':'1','tags':''}\n",
    "params['tags'] = movietype\n",
    "params_encode = urllib.parse.urlencode(params).encode('utf-8')\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "\n",
    "request = urllib.request.Request(url,data=params_encode,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"15 urllib通过已登录的cookie值，以登录用户身份访问网页\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "##demo1 通过已登录的cookie值，以登录用户身份访问网页\n",
    "url = 'http://www.renren.com/968196747/profile'\n",
    "\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "           \n",
    "          }\n",
    "'''\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "           'Cookie':'anonymid=jmn5bx3v-5le0e; depovince=GW; _r01_=1; ick_login=c9a5ab7b-7432-4e79-8d8d-d9e3f89ab72c; ick=a0121b31-6fe8-4eef-bf25-4278b6f19743; XNESSESSIONID=d51be5b82a20; WebOnLineNotice_968196747=1; JSESSIONID=abcISInMXWcrBd2AlxKyw; jebe_key=ed233682-75b8-496c-a199-557f631f200e%7Cc377fecba1c1e1def24233f88b207b06%7C1538208356017%7C1%7C1538208354004; wp_fold=0; jebecookies=dea9e704-78ec-4756-90d7-1a93dae3be67|||||; _de=D6104CF9DBA07C121FF9E00605E6865D; p=a9d3694434120963a494d947ab2906477; first_login_flag=1; ln_uact=13141055789; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=3395ed44534775ea30e64655670389627; societyguester=3395ed44534775ea30e64655670389627; id=968196747; xnsid=dba2a4e4; loginfrom=syshome',\n",
    "         }\n",
    "'''\n",
    "request = urllib.request.Request(url)#,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "    \n",
    "##demo2 使用更加机智的方法\n",
    "##    使用urllib.request.cookielib库和urllib.request.HTTPCookieProcessor处理器，自动收集cookie，不再手工抓包；\n",
    "\n",
    "import http.cookiejar #用于web客户端的http cookie处理，管理cookie值，存储http请求产生额cookie，向传出的请求添加cookie对象\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "cookie = http.cookiejar.CookieJar()\n",
    "\n",
    "cookieHandler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(cookieHandler)\n",
    "\n",
    "##demo1 通过已登录的cookie值，以登录用户身份访问网页\n",
    "import urllib.request\n",
    "import http.cookiejar\n",
    "import urllib.parse\n",
    "cookie = http.cookiejar.CookieJar()\n",
    "\n",
    "cookieHandler = urllib.request.HTTPCookieProcessor(cookie)\n",
    "opener = urllib.request.build_opener(cookieHandler)\n",
    "\n",
    "##demo1 通过已登录的cookie值，以登录用户身份访问网页\n",
    "#    以中国博士网为例 \n",
    "url = 'http://www.chinaphd.com/cgi-bin/loginout.cgi?forum='\n",
    "formdata = {'action':'login',\n",
    "            'forum':'',\n",
    "            'inmembername': 'hhhparty',\n",
    "            'inpassword':'tdzj1234',\n",
    "            'hidden':'0',\n",
    "            'CookieDate':'0',\n",
    "            'onlineview':'1',\n",
    "            'viewMode':'',\n",
    "            'selectstyle': '',\n",
    "            'tanchumsg':'',\n",
    "            'freshtime':'', \n",
    "           }\n",
    "data = urllib.parse.urlencode(formdata).encode('utf-8')\n",
    "\n",
    "opener.addheaders = [('User-Agent','Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36')]\n",
    "\n",
    "request = urllib.request.Request(url,data=data)#,headers=headers)\n",
    "response =  opener.open(request)\n",
    "print(response.read().decode('gbk'))\n",
    "\n",
    "#登录后，还可以继续访问其他页面\n",
    "response_message = opener.open('http://www.chinaphd.com/cgi-bin/messanger.cgi?action=inbox')\n",
    "print(response_message.read().decode('gbk'))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"访问https\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import ssl\n",
    "#尝试访问http://www.baidu.com 与https://www.baidu.com，观察它们的不同\n",
    "#再尝试访问12306网站 'http://www.12306.cn/mormhweb/' 与 https://www.12306.cn/mormhweb/的不同\n",
    "\n",
    "url = 'https://www.12306.cn/mormhweb/'\n",
    "\n",
    "#导入证书需要使用ssl库,可以查看urllib源码如何使用这个函数\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "\n",
    "#request = urllib.request.Request(url,unverifiable=True)\n",
    "request = urllib.request.Request(url)\n",
    "#分别尝试下列语句块，查看不同的结果。                             \n",
    "with urllib.request.urlopen(request,context=context) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "'''        \n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "'''\n",
    "'''\n",
    "with urllib.request.urlopen(url,cafile=cafile) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
