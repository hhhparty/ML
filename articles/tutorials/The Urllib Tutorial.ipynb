{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用urllib获取www资源\n",
    "\n",
    "目标：学习使用urllib编写简单爬虫程序，获取webpage资源\n",
    "\n",
    "\n",
    "1.使用urlopen()实现最简单的url访问，获取所指页面内容。\n",
    "\n",
    "2.使用urlretireve()将页面内容存为临时文件，并获取response头\n",
    "\n",
    "3.定制request对象，使爬虫更像浏览器\n",
    "    默认情况下，urllib发出的请求头如下所示（用wireshark工具截获可知）：\n",
    "        GET / HTTP/1.1\n",
    "        Accept-Encoding: identity\n",
    "        Host: 10.10.10.135\n",
    "        User-Agent: Python-urllib/3.6\n",
    "        Connection: close\n",
    "4.利用GET方法，向百度服务器发送查询请求，并获得查询结果 \n",
    "\n",
    "5.利用POST方法，向http://10.10.10.135/WebGoat/ 提交用户名和密码\n",
    "\n",
    "6.利用urllib.error处理异常,两个常用异常类：urllib.error.URLError和HTTPError\n",
    "\n",
    "7.认识response类的方法：info(),geturl()\n",
    "\n",
    "8.一个较为完整case，从百度贴吧下载多页话题\n",
    "\n",
    "9.通过HTTP basic authentication\n",
    "    登陆网页前遇到的要求输入用户名和密码的程序，通常称为身份认证程序。\n",
    "    HTTP认证可以保护一个作用域（称为一个realm）内的资源不受非法访问。\n",
    "    HTTP规范中定义了两种认证模式：basic auth和digest auth\n",
    "    认证的基本过程是：1.客户请求访问网页；2.服务器端返回401错误，要求认证；\n",
    "    3.客户端重新提交请求并附以认证信息，这部分信息将被编码；\n",
    "    4.服务器检查信息，通过则给以正常服务页面；否则返回401错误。\n",
    "    \n",
    "   第一次服务器返回401错误时，会返回headers字典信息，其中会包含如下信息：\n",
    "    WWW-Authenticate: Basic realm=\"cPanel\"\n",
    "    我们假定已知用户名和密码，之后利用一定的编码格式将realm名、用户名、密码等信息\n",
    "    编码后就可以传递给服务器，认证就可通过。\n",
    "    编码格式是base-64\n",
    "\n",
    "10.使用代理\n",
    "11.设置time-out\n",
    "12.使用HEAD方法，请求服务器\n",
    "\n",
    "13.urllib加载ajax信息\n",
    "   AJAX = Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）。\n",
    "   AJAX 最大的优点是在不重新加载整个页面的情况下，可以与服务器交换数据并更新部分网页内容。\n",
    "   AJAX 不需要任何浏览器插件，但需要用户允许JavaScript在浏览器上执行。\n",
    "    \n",
    "   这里以 https://movie.douban.com/tag/#/ 为例\n",
    "   先使用抓包工具查看一下这个页面，通过测试可以发现每次点击“更多”会增加一个响应    \n",
    "   https://movie.douban.com/j/new_search_subjects?sort=U&range=0,10&tags=&start=40\n",
    "   将其直接在浏览器中打开，可以看到它以json格式记录了新加载的电影信息。\n",
    "   找到这个文件后，就可开始尝试了。\n",
    "14.urllib通过已登录的cookie值，以登录用户身份访问网页。\n",
    "    首先用浏览器登录，获取登陆后的cookie，通常这个cookie会非常长。\n",
    "    我们以访问http://www.renren.com/968196747/profile 这个登录后链接为例 如果成功会得到相关内容.\n",
    "    将cookkie值作为字符串加载在headers里。\n",
    "    \n",
    "    \n",
    "15.访问https站点\n",
    "    需要CA证书才能访问,证书是用于加密连接和身份认证的数字凭据，通常由公信机构发放。\n",
    "    尝试访问http://www.baidu.com 与https://www.baidu.com， 观察它们的不同\n",
    "    百度访问https时会有跳转。\n",
    "    再尝试访问12306网站 'http://www.12306.cn/mormhweb/' 与 https://www.12306.cn/mormhweb/ 的不同\n",
    "    可以看到访问 https://www.12306.cn/mormhweb/ 时会报出错误：CertificateError: hostname 'www.12306.cn' doesn't match either of 'webssl.chinanetcenter.com'\n",
    "    ssl库ssl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"使用urlopen()实现最简单的url访问\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "\n",
    "url = 'http://10.10.10.135/'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    print(response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"使用urlretireve()将页面内容存为临时文件，并获取response头\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "\n",
    "url = 'http://10.10.10.135/'\n",
    "localfile, headers = urllib.request.urlretrieve(url)\n",
    "print(localfile)\n",
    "print()\n",
    "print(headers)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"定制request对象，使爬虫更像浏览器\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "\n",
    "url = 'http://10.10.10.135/'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"使用GET方法，向百度服务器发送查询请求\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "querystr = {'wd':'北航'}\n",
    "querystr_encode = urllib.parse.urlencode(querystr)\n",
    "print(querystr_encode)\n",
    "#https://www.baidu.com/s?wd=%E5%8C%97%E8%88%AA\n",
    "url = 'http://www.baidu.com/s?' + querystr_encode\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n",
    "    print(response.headers)\n",
    "    html = response.read()\n",
    "    print(html.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"利用POST方法，向http://10.10.10.135/WebGoat/ 提交用户名和密码\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = 'http://10.10.10.135/dvwa/login.php'\n",
    "cookie = 'PHPSESSID=898c1rsum58475qh3nros002n7; path=/'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "         'Cookie':cookie,\n",
    "          }\n",
    "authstr = {'username':'admin',\n",
    "           'password':'admin',\n",
    "           'Login':'Login',}\n",
    "data = urllib.parse.urlencode(authstr).encode('utf-8')\n",
    "\n",
    "request = urllib.request.Request(url,data=data,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n",
    "    print(response.headers)\n",
    "    cookie1 = response.headers['Set-Cookie']\n",
    "\n",
    "url = 'http://10.10.10.135/dvwa/index.php'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          'Cookie':cookie+';'+cookie1,\n",
    "          }\n",
    "print(headers)\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"使用urlllib.error处理异常\n",
    "URLError继承自OSError，是urllib的异常的基础类\n",
    "HTTPError是验证HTTP response实例的一个异常类。\n",
    "\n",
    "HTTP protocol errors是有效的response，有状态码、headers、body。\n",
    "\n",
    "一个成熟的程序需要管理所有输出，不仅有希望见到的输出，还要有意料之外的异常。\n",
    "logging的使用可以参考https://docs.python.org/3.5/howto/logging.html\n",
    "\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    filename='C:\\\\Users\\\\leo\\Documents\\\\crawlerslesson1_crawler.log',\n",
    "                    level=logging.DEBUG)\n",
    "try: \n",
    "    #url = 'http://www.baidu11.com'\n",
    "    url = 'http://10.10.10.135/WebGoat/attack'\n",
    "    headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "    request = urllib.request.Request(url,headers=headers)\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        print(response.status)\n",
    "        print(response.read().decode('utf-8'))\n",
    "\n",
    "except urllib.error.HTTPError as e:\n",
    "    import http.server\n",
    "    #print(http.server.BaseHTTPRequestHandler.responses[e.code])\n",
    "    logging.error('HTTPError code: %s and Messages: %s'% (str(e.code),http.server.BaseHTTPRequestHandler.responses[e.code]))\n",
    "    logging.info('HTTPError headers: ' + str(e.headers))\n",
    "    logging.info(e.read().decode('utf-8'))\n",
    "    print('不好意思，服务器卡壳儿了，请稍后重试。')\n",
    "except urllib.error.URLError as e:\n",
    "    logging.error(e.reason)\n",
    "    print('不好意思，服务器卡壳儿了，请稍后重试。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"使用response的geturl和info方法来验证访问是否相符\n",
    "geturl - this returns the real URL of the page fetched. \n",
    "This is useful because urlopen (or the opener object used) may have followed a redirect. \n",
    "The URL of the page fetched may not be the same as the URL requested.\n",
    "\n",
    "info - this returns a dictionary-like object that describes the page fetched,\n",
    "particularly the headers sent by the server.\n",
    "It is currently an http.client.HTTPMessage instance.\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "url = 'http://www.baidu.com'\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    #print(type(response))\n",
    "    print(response.info())\n",
    "    print(response.geturl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A case of crawler is used to fetch the content of baidu's tieba url, in according to user's input keywords.\n",
    "\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "def loadPage(url):\n",
    "    \"\"\"\n",
    "        Function: Fetching url and accessing the webpage content.\n",
    "        url: the wanted webpage url.\n",
    "    \"\"\"\n",
    "    headers = {'Accept': 'text/html','User-Agent':'Mozilla/5.0',}\n",
    "    print('To send http request to %s' % url)\n",
    "    request = urllib.request.Request(url,headers=headers)\n",
    "\n",
    "    return  urllib.request.urlopen(request).read()\n",
    "\n",
    "def writePage(html,filename):\n",
    "    \"\"\"\n",
    "        Fuction: To write the content of html into a local file.\n",
    "        html: The response content.\n",
    "        filename: the local filename to be used stored the response.\n",
    "    \"\"\"\n",
    "    print('To write html into a local file %s ...' % filename)\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(str(html))\n",
    "    print('Work done.')\n",
    "    print('-'*10)\n",
    "\n",
    "def tiebaCrawler(url,beginpPage,endPage,keyword):\n",
    "    \"\"\"\n",
    "        Function: The scheduler of tieba crawler, is used to access every wanted url in turns.\n",
    "        url: the url of baidu's tieba webpage\n",
    "        beginPage: initial page\n",
    "        endPage: end page\n",
    "        keyword: the wanted keyword \n",
    "    \"\"\"\n",
    "    filename = keyword + '_tieba.html'\n",
    "    for page in range(beginpPage,endPage+1):\n",
    "        pn = (page - 1) * 50\n",
    "        queryurl = url + '&pn=' + str(pn)\n",
    "        #print(queryurl)\n",
    "        \n",
    "        writePage(loadPage(queryurl),filename)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    kw = input('Pl input the wanted tieba\\'s name:' )\n",
    "    beginPage = int(input('The beginning page number:'))\n",
    "    endPage = int(input('The ending page number:'))\n",
    "    url = 'http://tieba.baidu.com/f?'\n",
    "    key = urllib.parse.urlencode({'kw':kw})\n",
    "    queryurl = url+ key\n",
    "    tiebaCrawler(url,beginPage,endPage,kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"An case of passing basic authentication with urllib\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import logging\n",
    "\n",
    "url = 'http://10.10.10.135/WebGoat/attack'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "\n",
    "def passBasicAuth(realm):\n",
    "\n",
    "    import base64\n",
    "    username = 'webgoat'\n",
    "    password = 'webgoat'\n",
    "    bstr = username+':'+password\n",
    "    schemastr , realmname = realm.split('=')\n",
    "    if schemastr.lower().find('basic') >= 0:\n",
    "        schema ='Basic'\n",
    "    else:\n",
    "        print('The authentication schema isn\\'t basic, programe exit.')\n",
    "        exit(-1)\n",
    "        \n",
    "    base64str = base64.b64encode(bstr.encode('utf-8'))\n",
    "    authheader = 'Basic %s' % base64str.decode('utf-8')\n",
    "    \n",
    "    request.add_header('Authorization',authheader)\n",
    "    print(request.headers)\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        print(response.status)\n",
    "        print(response.read().decode('utf-8'))\n",
    "    \n",
    "     \n",
    "    \n",
    "try:\n",
    "    #url = 'http://www.baidu11.com'\n",
    "    \n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        print(response.status)\n",
    "        print(response.info())\n",
    "        #print(response.read().decode('utf-8'))\n",
    "except urllib.error.URLError as e:\n",
    "    if  hasattr(e,'code'):\n",
    "        \n",
    "        print(e.code)\n",
    "        print(e.info())\n",
    "        if e.code == 401:\n",
    "            passBasicAuth(e.headers['WWW-Authenticate'])            \n",
    "    elif hasattr(e,'reason'):\n",
    "        print(e.reason)\n",
    "    else:\n",
    "        print('unkown error.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"An case of passing basic authentication with urllib\n",
    "\n",
    "Worked!\n",
    "\n",
    "\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "\n",
    "url = 'http://10.10.10.135/WebGoat/attack'\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "request = urllib.request.Request(url,headers=headers)\n",
    "username = 'webgoat'\n",
    "password = 'webgoat'\n",
    "\n",
    "passman = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "# this creates a password manager\n",
    "passman.add_password(None, url, username, password)\n",
    "# because we have put None at the start it will always\n",
    "# use this username/password combination for  urls\n",
    "# for which `theurl` is a super-url\n",
    "\n",
    "authhandler = urllib.request.HTTPBasicAuthHandler(passman)\n",
    "# create the AuthHandler\n",
    "\n",
    "opener = urllib.request.build_opener(authhandler)\n",
    "\n",
    "urllib.request.install_opener(opener)\n",
    "# All calls to urllib2.urlopen will now use our handler\n",
    "# Make sure not to include the protocol in with the URL, or\n",
    "# HTTPPasswordMgrWithDefaultRealm will be very confused.\n",
    "# You must (of course) use it when fetching the page though.\n",
    "\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.status)\n",
    "    print(response.read().decode('utf-8'))\n",
    "# authentication is now handled automatically for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"使用代理\"\"\"\n",
    "import urllib.request\n",
    "proxy_support = urllib.request.ProxyHandler({'sock5': 'localhost:1080'})\n",
    " \n",
    "opener = urllib.request.build_opener(proxy_support)\n",
    " \n",
    "urllib.request.install_opener(opener)\n",
    " \n",
    "a = urllib.request.urlopen(\"http://www.python.org/\").read().decode(\"utf8\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"设置time-out\"\"\"\n",
    "import socket\n",
    "import urllib.request\n",
    "# timeout in seconds\n",
    "timeout = 2\n",
    "socket.setdefaulttimeout(timeout)\n",
    "# this call to urllib.request.urlopen now uses the default timeout\n",
    "# we have set in the socket module\n",
    "req = urllib.request.Request('http://www.python.org/')\n",
    "a = urllib.request.urlopen(req).read()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"抓取ajax页面\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "url = 'https://movie.douban.com/j/new_search_subjects?'\n",
    "movietype = '动作'\n",
    "params = {'sort':'U','range':'0,10','tags':'','start':'1','tags':''}\n",
    "params['tags'] = movietype\n",
    "params_encode = urllib.parse.urlencode(params).encode('utf-8')\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "          }\n",
    "\n",
    "request = urllib.request.Request(url,data=params_encode,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"urllib通过已登录的cookie值，以登录用户身份访问网页\"\"\"\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "url = 'http://www.renren.com/968196747/profile'\n",
    "\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "           \n",
    "          }\n",
    "'''\n",
    "headers = {'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n",
    "           'Cookie':'anonymid=jmn5bx3v-5le0e; depovince=GW; _r01_=1; ick_login=c9a5ab7b-7432-4e79-8d8d-d9e3f89ab72c; ick=a0121b31-6fe8-4eef-bf25-4278b6f19743; XNESSESSIONID=d51be5b82a20; WebOnLineNotice_968196747=1; JSESSIONID=abcISInMXWcrBd2AlxKyw; jebe_key=ed233682-75b8-496c-a199-557f631f200e%7Cc377fecba1c1e1def24233f88b207b06%7C1538208356017%7C1%7C1538208354004; wp_fold=0; jebecookies=dea9e704-78ec-4756-90d7-1a93dae3be67|||||; _de=D6104CF9DBA07C121FF9E00605E6865D; p=a9d3694434120963a494d947ab2906477; first_login_flag=1; ln_uact=13141055789; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=3395ed44534775ea30e64655670389627; societyguester=3395ed44534775ea30e64655670389627; id=968196747; xnsid=dba2a4e4; loginfrom=syshome',\n",
    "         }\n",
    "'''\n",
    "request = urllib.request.Request(url)#,headers=headers)\n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"访问https\"\"\"\n",
    "\n",
    "import urllib.request\n",
    "import ssl\n",
    "#尝试访问http://www.baidu.com 与https://www.baidu.com，观察它们的不同\n",
    "#再尝试访问12306网站 'http://www.12306.cn/mormhweb/' 与 https://www.12306.cn/mormhweb/的不同\n",
    "\n",
    "url = 'https://www.12306.cn/mormhweb/'\n",
    "\n",
    "#导入证书需要使用ssl库,可以查看urllib源码如何使用这个函数\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "\n",
    "#request = urllib.request.Request(url,unverifiable=True)\n",
    "request = urllib.request.Request(url)\n",
    "#分别尝试下列语句块，查看不同的结果。                             \n",
    "with urllib.request.urlopen(request,context=context) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "'''        \n",
    "with urllib.request.urlopen(request) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "'''\n",
    "'''\n",
    "with urllib.request.urlopen(url,cafile=cafile) as response:\n",
    "    print(response.read().decode('utf-8'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?urllib.request.urlopen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
