{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "\n",
    "## 神经元模型\n",
    "\n",
    "神经网络（neural networks）方面的研究很早就出现了，今天“神经网络”已经是一个相当大的、多学科交叉的学科领域。\n",
    "\n",
    "各种相关学科对神经网络的定义多种多样，我们采用目前使用的最广泛的一种：\n",
    "\n",
    "**神经网络是由具有适应性的、简单单元组成的、广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反映。**[Koheonen，1988]\n",
    "\n",
    "我们在机器学习中谈论神经网络时，指的是“神经网络学习”，或者说是，机器学习于神经网络这两个学科领域的交叉部分。\n",
    "\n",
    "**神经网络中，最基本的成分是神经元（neuron）模型**，即上述定义中的“简单单元”。\n",
    "\n",
    "![生物神经元](images/neuralnetwork/生物神经元.jpg)\n",
    "\n",
    "![兴奋的生物神经元](images/neuralnetwork/兴奋的生物神经元.jpg)\n",
    "\n",
    "在生物神经网络中，每个神经元与其它神经元相连，当他兴奋时，就会像相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个“阈值”（threshold）也称为bias，那么他就会被激活，即“兴奋”起来，像其它神经元发送化学物质。\n",
    "\n",
    "1943年，[McCulloch and Pitts，1943]将上述情景抽象为下图所示的简单模型。\n",
    "\n",
    "![mp神经元模型](images/neuralnetwork/mp神经元模型.png)\n",
    "\n",
    "\n",
    "这就是沿用至今的“M-P神经元模型”。在这个模型中，神经元接收到来自n个其它神经元传递过来的输入信号，这些输入信号通过带权重的链接（connection）进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过“激活函数”（activation function)处理以产生神经元的输出。\n",
    "\n",
    "理想中的激活函数是下图5.2a所示的阶跃函数，它将输入值映射为输出值“0”或“1”，显然：\n",
    "\n",
    "- “1”对应于神经元兴奋\n",
    "- “0”对应于神经元抑制\n",
    "\n",
    "然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用Sigmoid函数如图5.2b所示，他把可能在较大范围内变化的输入值挤压到（0，1）输出值范围内，因此有时也成为“挤压函数”（squashing function）。\n",
    "\n",
    "![典型神经元激活函数](images/neuralnetwork/典型神经元激活函数.png)\n",
    "\n",
    "把许多的神经元按一定的层次结构连接起来，就得到了神经网络。\n",
    "\n",
    "事实上，从计算机科学角度看，我们可以先不考虑神经网络是否真的模拟了生物神经网络，只需将一个神经网络视为包含了许多参数的数学模型，这个模型是若干个函数，例如: $y_i = f(\\sum_i w_ix_i -\\theta_j)$ 相互（嵌套）代入而得。\n",
    "\n",
    "例如：10个神经元两两连接，则有100个参数：90个连接权重和10个阈值。\n",
    "\n",
    "有效的神经网络学习算法大多以数学证明为支撑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机与多层网络\n",
    "\n",
    "感知机（Perceptron）由两层神经元组成，如下图所示：\n",
    "\n",
    "输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称“阈值逻辑单元”（threshold logic unit）。\n",
    "\n",
    "感知机能容易地实现逻辑与、或、非运算。注意到 $y_i = f(\\sum_i w_ix_i -\\theta_j)$ ，假定f是图5.2中的阶跃函数，有：\n",
    "\n",
    "- “与”（$x_1 \\cap x_2 $）: 令 $w_1 = w_2 = 1,\\theta = 2$, $则 y = f(1 \\centerdot x_1 + 1 \\centerdot x_2 - 2)$，仅在$x_1 = x_2 = 1时，y=1；$\n",
    "- “或”（$x_1 \\cup x_2$）：令$w_1 = w_2 = 1, \\theta = 0.5$，$则 y = f(1 \\centerdot x_1 + 1 \\centerdot x_2 - 0.5), 当 x_1 = 1 或 x_2 = 1时，y=1$；\n",
    "- “非”（$-x_1$）：令$w_1 = -0.6，w_2 = 0，\\theta = - 0.5$，$则 y = f( -0.6 \\centerdot x_1 + 0 \\centerdot x_2 + 0.5)$,$当 x_1 = 1，y = 0 ; 当 x_1 = 0，y = 1 。$\n",
    "\n",
    "\n",
    "![两个输入神经元的感知机网络结构示意图](images/neuralnetwork/两个输入神经元的感知机网络结构示意图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更一般地，给定训练数据集，权重$w_i(i=1,2,...,n)$以及阈值$\\theta$可通过学习得到。\n",
    "\n",
    "阈值$\\theta$可看作一个固定输入为 -1.0 的**哑节点（dummy node）**所对应的连接权重$w_{n+1}$，这样，权重和阈值的学习就可以统一为权重的学习。\n",
    "\n",
    "感知机学习规则非常简单，对训练样例（x,y），若当前感知机的输出为$\\hat y$，则感知机权重将这样调整：\n",
    "\n",
    "$w_i \\gets w_i  +  \\Delta w_i$  ——式（1）\n",
    "\n",
    "$\\Delta w_i = \\eta(y - \\hat y)x_i$ ——式（2）\n",
    "\n",
    "其中，$\\eta \\in (0,1)$称为学习率（learning rate）。从式（1）可以看出，若感知机对训练样例（x,y）预测正确，即$\\hat y = y$，则感知机不发生变化，否则将根据错误的程度进行权重调整。\n",
    "\n",
    "需注意的是，感知机只有输出层神经元进行激活函数处理，即只有一层功能神经元（functional neuron），其学习能力非常有限。\n",
    "\n",
    "事实上，上述与、或、非问题都是线性可分（linear separable）的问题。可以证明，若两类模式是线性可分的，即存在一个线性超平面能将它们分开，如5.4（a)-(c)所示，则感知机的学习过程一定会收敛（converge）而求得适当的权向量 $w = (w_1;w_2;...;w_{n+1})$；否则感知机学习过程将会发生振荡（fluctuation），$w$难以稳定下来，不能求得合适解，例如感知机甚至不能解决如图5.4(d)所示的异或这样简单的非线性可分问题。\n",
    "\n",
    "![线性可分或不可分示意图](images/neuralnetwork/线性可分或不可分示意图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要解决非线性可分问题，需要使用多层功能神经元。例如图5.5重这个简单的两层感知机就能解决异或问题。\n",
    "\n",
    "在图5.5a中，输出层与输入层之间的一元神经元，被称为**隐层或隐含层（hidden layer）**，隐含层和输出层都是拥有激活函数的功能神经元。\n",
    "\n",
    "![能解决异或问题的两层感知机](images/neuralnetwork/能解决异或问题的两层感知机.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更一般的，常见的神经网络是形如图5.6所示的层级结构，每层神经元与下一层神经元全连接，神经元间不存在同层连接，也不存在跨层连接。\n",
    "\n",
    "这样的神经网络结构通常称为“多层前馈神经网络”（multi-layer feedforward neural networks），其中输入层神经元接收外界输入，隐层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。\n",
    "\n",
    "![多层前馈神经网络结构示意图](images/neuralnetwork/多层前馈神经网络结构示意图.png)\n",
    "\n",
    "换言之，**输入神经元，仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元。**\n",
    "\n",
    "因此，图5.6a通常被称为“两层网络”。为避免歧义，我们可称其为“单隐层网络”。只需包含隐层，就可成为多层网络。\n",
    "\n",
    "神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”（connection weight）以及每个功能神经元的阈值；换言之，神经网络“学到”的东西，蕴含在连接权与阈值中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差逆传播算法（BP神经网络）\n",
    "\n",
    "多层网络的学习能力比单层感知机强得多。\n",
    "\n",
    "欲训练多层网络，式（1）的简单感知机学习规则显然不够了，需要更强大的学习算法。\n",
    "\n",
    "误差逆传播（Error BackPropagation，简称BP）算法就是其中最杰出的代表，它是迄今最成功的神经网络学习算法。\n",
    "\n",
    "现实任务中使用的神经网络时，大多是在使用BP算法进行训练。\n",
    "\n",
    "值得指出的是，BP算法不仅可用于多层前馈神经网络，还可用于其它类型的神经网络，一般是指用BP算法训练的多层前馈神经网络。\n",
    "\n",
    "下面我们来看看BP算法究竟是什么样的。\n",
    "\n",
    "给定训练集$D = {(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}, x_i \\in R^d ,y_i \\in R^l$，即输入示例由$d$个属性描述，输出$l$维实值向量。\n",
    "\n",
    "为了方便讨论，图5.7给出了一个拥有 d 个输入神经元，$l$个输出神经元、$q$个隐层神经元的多层前馈网络结构。其中，输出层第 j 个神经元的阈值用$\\theta_j$表示，隐层第 h 个神经元的阈值用 $\\gamma_h$表示。\n",
    "\n",
    "![BP网络及算法中的变量符号](images/neuralnetwork/BP网络及算法中的变量符号.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入层第 i 个神经元与隐层第 h 个神经元之间的连接权为 $v_{ih}$，隐层第 h 个神经元与输出层第 j 个神经元之间的连接权为 $w_{hj}$。\n",
    "\n",
    "记隐层第 h 个神经元接收到的输入为 $\\alpha_h = \\sum_{i=1}^d v_{ih}x_i$，输出层第 j 个神经元接收到的输入为 $\\beta_j = \\sum_{h=1}^q w_{hj}b_h$，其中$b_h$为隐层第h个神经的输出。假设隐层和输出层神经元都是用图5.2b中的Sigmoid函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对训练样例$(x_k,y_k)$,假定神经网络的输出为$\\hat y_k = (\\hat y_1^k,\\hat y_2^k,...,\\hat y_l^k)$，即：\n",
    "\n",
    "$\\hat y_j^k = f(\\beta_j - \\theta_j)$  ——式（3）\n",
    "\n",
    "则网络在$(x_k,y_k)$上的均方误差为：\n",
    "\n",
    "$ E_k = \\frac{1}{2 l}\\sum_{j=1}^{l}(\\hat y_j^k - y_j^k)^2 = f(\\beta_j - \\theta_j)$  ——式（3）\n",
    "\n",
    "注：1/2 是为了后续求导方便。\n",
    "\n",
    "图5.7的网络中有 $(d+l+1)q+l$ 个参数需确定：输入层到隐层的$d \\times q$个权值、隐层到输出层的$q \\times l$个权重、q 个隐层神经元的阈值、l 个输出层神经元的阈值。\n",
    "\n",
    "BP是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，即与式（1）类似，任意参数 $v$ 的更新估计式为：\n",
    "\n",
    "$v \\gets  v  +  \\Delta v$  ——式（5）\n",
    "\n",
    "下面，我们可以图5.7中隐层到输出层的连接权$w_{hj}$为例进行推导。\n",
    "\n",
    "**BP算法基于梯度下降（gradient descent）策略，以目标的负梯度方向对参数进行调整**。对式（4）的误差$E_k$，给定学习率$\\eta$，有：\n",
    "\n",
    "$\\Delta w_{hj} = - \\eta \\frac{\\partial E_k}{\\partial w_{hj}}$  ——式（6）\n",
    "\n",
    "注意到$w_{hj}$先影响到第 j 个输出层神经元的输入值 $\\beta_j$，再影响到其输出值$\\hat y_j^k$，然后影响到$E_k$，有：\n",
    "\n",
    "$\\frac{\\partial E_k}{\\partial w_{hj}} = \\frac{\\partial E_k}{\\partial \\hat y_j^k} \\centerdot \\frac{\\partial \\hat y_j^k}{\\partial \\beta _j} \\centerdot \\frac{\\partial \\beta _j}{\\partial w_{hj}}$  ——式（7）\n",
    "\n",
    "根据$\\beta_j$的定义，显然有：\n",
    "\n",
    "$\\frac{\\partial \\beta _j}{\\partial w_{hj}} = b_h$  ——式（8）\n",
    "\n",
    "图5.2中的Sigmoid函数有一个很好的性质：\n",
    "\n",
    "$f^{'}(x) = f(x)(1-f(x))$ ——式（9）\n",
    "\n",
    "于是根据式（4）和（3），有：\n",
    "\n",
    "$g_j = - \\frac{\\partial E_k}{\\partial \\hat y_j^k} \\centerdot \\frac{\\partial \\hat y_j^k}{\\partial \\beta _j}  = -(\\hat y_j^k - y_j^k)f^{'}(\\beta_j -\\theta_j) = \\hat y_j^k(1-\\hat y_j^k)(y_j^k- \\hat y_j^k)$ ——式（10）\n",
    "\n",
    "将式（10）、式（8）代入式（7），再代入式（6），就得到了BP算法中关于$w_{hj}$的更新公式：\n",
    "\n",
    "$\\Delta w_{hj} = \\eta g_j b_h$ ——式（11）\n",
    "\n",
    "类似可得：\n",
    "\n",
    "$\\Delta \\theta_j = - \\eta g_j $ ——式（12）\n",
    "\n",
    "$\\Delta v_{jh} = \\eta e_h x_i $ ——式（13）\n",
    "\n",
    "$\\Delta \\gamma_h = \\eta e_h $ ——式（14）\n",
    "\n",
    "式（13）和（14）中，\n",
    "\n",
    "$e_h = - \\frac{\\partial E_k}{\\partial b_h} \\centerdot \\frac{\\partial b_h}{\\partial \\alpha_h} = - \\sum_{j=1}^l \\frac{\\partial E_k}{\\partial \\beta_j} \\centerdot \\frac{\\partial \\beta_j}{\\partial b_h} f^{'}(\\alpha_h - \\gamma_h) = \\sum_{j=1}^{l}w_{hj}g_jf^{'}(\\alpha_h - \\gamma_h) = b_h(1-b_h)\\sum_{j=1}^{l}w_{hj}g_j$ ——式（15）\n",
    "\n",
    "学习率$\\eta \\in (0,1)$控制着算法每一轮迭代中的更新步长，若太大则容易振荡，太小则收敛速度又会过慢。\n",
    "\n",
    "有时为了做精细调节，可令式（11）与（12）使用$\\eta_1$，式（13）与（14）使用$\\eta_2$，两者未必相等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP算法的工作流程\n",
    "\n",
    "对于每个训练样例，BP算法执行以下操作：\n",
    "\n",
    "![标准BP算法](images/neuralnetwork/标准BP算法.png)\n",
    "\n",
    "- 先将输入示例提供给输入层神经元；\n",
    "- 然后逐层将信号前传，直到产生输出层的结果；\n",
    "- 然后计算输出层的误差，再将误差逆向传播至隐层神经元；\n",
    "- 最后根据隐层神经元的误差来对连接权和阈值进行调整；\n",
    "- 该迭代过程循环进行，直到达到某些停止条件为止。例如训练误差已达到一个很小的值。\n",
    "\n",
    "下图，给出了在2个属性、5个样本的西瓜数据集上，随着训练轮数的增加，网络参数和分类边界的变化情况：\n",
    "\n",
    "![2个属性5个样本BP网络示例](images/neuralnetwork/2个属性5个样本BP网络示例.png)\n",
    "\n",
    "需要注意的是，BP算法的目标是要最小化训练集D上的累积误差：\n",
    "\n",
    "$E = \\frac{1}{m}\\sum_{k=1}^m E_k$  ——式（16）\n",
    "\n",
    "但我们介绍的“标准BP算法”每次仅针对一个训练样例更新连接权和阈值，也就是说，上述算法的更新规则是基于单个的$E_k$推导而得。\n",
    "\n",
    "如果类似地推导出基于累积误差最小化地更新规则，就得到了累积误差逆传播（accumulated error backpropagation）算法。累积BP算法与标准BP算法都很常用。\n",
    "\n",
    "一般说来，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新得效果可能出现“抵消”现象。\n",
    "\n",
    "因此，为了达到累积误差极小点，标准BP算法往往需要进行更多次数得迭代。\n",
    "\n",
    "累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后，才对参数进行更新，其参数更新的频率地得到。\n",
    "\n",
    "在很多任务中，累积误差下降到一定程度后，进一步下降会非常缓慢，这是标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hornik et al. 1989]证明，只需要一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数。然而，如何设置隐层神经元的个数，仍是一个未决的问题，实际应用中通常靠“试错法”（trial-by-error）调整。\n",
    "\n",
    "正是由于其强大的表示能力，BP神经网络经常遭遇过拟合。其训练误差持续降低，但测试误差却仍可能上升。\n",
    "\n",
    "有两种策略，常用来缓解BP网络的过拟合：\n",
    "\n",
    "- 第一种策略是“早停（early stopping）”，即将数据分为训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差减低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n",
    "\n",
    "- 第二种策略是“正则化（regularization）”，其最基本思想是误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权与阈值的平方和。仍令$E_k$表示第k个训练样例上的误差，$w_i$表示连接权和阈值，则误差目标函数（16）改变为：\n",
    "\n",
    "$E = \\lambda \\frac{1}{m} \\sum_{k=1}^m E_k +(1-\\lambda)\\sum_i w_i^2$ ——式（17）\n",
    "\n",
    "其中$\\lambda \\in (0,1)$用于对经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他常见神经网络\n",
    "\n",
    "神经网络模型、算法繁多，常见的有：\n",
    "\n",
    "### RBF网络\n",
    "\n",
    "RBF（Radial Basis Function，径向基函数）网络是一种单隐层前馈神经网络，它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合。\n",
    "\n",
    "### ART网络\n",
    "\n",
    "竞争型学习（competitive learning）是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其它神经元的状态被抑制。这种机制亦称“胜者通吃”（winner take all）原则。\n",
    "\n",
    "ART（Adaptive Resonance Theory，自适应谐振理论）网络是竞争型学习的重要代表。\n",
    "\n",
    "该网络由比较层、识别层、识别阈值和重叠模块构成。\n",
    "\n",
    "其中，比较层负责接收输入样本，并将其传递给识别层神经元。识别层每个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类。\n",
    "\n",
    "### Boltzmann机\n",
    "\n",
    "神经网络中有一类模型是为网络状态定义一个“能量”（energy），能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数。\n",
    "\n",
    "Boltzmann机就是一种“基于能量的模型”（energy-based model），常见结构如下图所示，器神经元分为两层：显层与隐层。\n",
    "\n",
    "![Boltzmann机与受限Boltzmann机](images/neuralnetwork/Boltzmann机与受限Boltzmann机.png)\n",
    "\n",
    "显层用于表示数据的输入与输出，隐层则被理解为数据的内在表达。\n",
    "\n",
    "Boltzmann机种的神经元都是布尔型的，即只能取0、1两种状态，状态1表示激活，状态0表示抑制。\n",
    "\n",
    "令向量$s \\in {0,1}^n$表示 n 个神经元的状态，$w_{ij}$表示神经元 i 与 j 之间的连接权，$\\theta_i$表示神经元 i 的阈值，则状态向量 s 所对应的 Boltzmann机能量定义为：\n",
    "\n",
    "$E(s) = - \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}s_is_j - \\sum_{i=1}^{n}\\theta_i s_i$  ——式（20）\n",
    "\n",
    "若网络中的神经元一任意不依赖于输入值的顺序进行更新，则网络最终将达到Boltzmann分布，此时状态向量 s 出现的概率将仅由其能量与所有可能状态向量的能量确定：\n",
    "\n",
    "$P(s) = \\frac{e^{-E(s)}}{\\sum_{t}e^{-E(t)}}$ ——式（21）\n",
    "\n",
    "Boltzmann机的训练过程就是将每个训练样本视为一个状态向量，使其出现的概率尽可能答。\n",
    "\n",
    "标准Boltzmann机是一个全连接图，训练网络的复杂度很高，这使其难以用于解决现实任务。\n",
    "\n",
    "现实中常用Boltzmann机（Restricted Boltzmann Machine,简称RBM）。\n",
    "\n",
    "如上图中，RBM机仅保留显层与隐层之间的连接，从而将Boltzmann机结构由完全图简为二部图。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用sklearn中的多层感知器进行分类和回归\n",
    "\n",
    "多层感知器MLP是一种监督学习算法，通过在数据集上训练来学习函数$f:X \\to y$,其中 X 是n维的输入，y是输出。\n",
    "\n",
    "中间隐藏层可能是多层。激活函数可以是双曲正切函数tanh。输出层接收到最后一个隐藏层的输出，经过变换得到y。\n",
    "\n",
    "sklearn中MLP模型，包含权重矩阵coefs_和阈值向量intercepts_。\n",
    "\n",
    "它的优点：\n",
    "\n",
    "- 可以学习到非线性模型，既可用于分类，也可用于回归；\n",
    "\n",
    "- 使用“partial_fit”可以学习得到实时模型（在线学习）\n",
    "\n",
    "它的缺点：\n",
    "\n",
    "- 具有隐藏层的 MLP 具有非凸的损失函数，它有不止一个的局部最小值。 因此不同的随机权重初始化会导致不同的验证集准确率。\n",
    "- MLP 需要调试一些超参数，例如隐藏层神经元的数量、层数和迭代轮数。\n",
    "- MLP 对特征归一化很敏感.\n",
    "\n",
    "## 分类任务\n",
    "\n",
    "MLPClassifier 类实现了通过 Backpropagation 进行训练的多层感知器（MLP）算法。\n",
    "\n",
    "下面举例说明：\n",
    "\n",
    "MLP 在两个 array 上进行训练:尺寸为 (n_samples, n_features) 的 array X 储存表示训练样本的浮点型特征向量; 尺寸为(n_samples,) 的 array y 储存训练样本的目标值（类别标签）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "clf.fit(X, y)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拟合（训练）后，该模型可以预测新样本的标签:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[2., 2.], [-1., -2.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP 可以为训练数据拟合一个非线性模型。clf.coefs_ 包含了构建模型的权值矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (5, 2), (2, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[coef.shape for coef in clf.coefs_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前， MLPClassifier 只支持交叉熵损失函数，通过运行 predict_proba 方法进行概率估计。\n",
    "MLP 算法使用的是反向传播的方式。 更准确地说，它使用了通过反向传播计算得到的梯度和某种形式的梯度下降来进行训练。 对于分类来说，它最小化交叉熵损失函数，为每个样本 X 给出一个向量形式的概率估计 $P(y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归\n",
    "\n",
    "sklearn中提供了一个MLPRegressor实现神经网络回归，该模型使用LBFGS或随机梯度下降来优化平方损失。\n",
    "\n",
    "MLPRegressor 类实现了一个多层感知器（MLP），它在使用反向传播进行训练时的输出层没有使用激活函数，也可以看作是使用身份函数作为激活函数。 因此，它使用平方误差作为损失函数，输出是一组连续值。\n",
    "\n",
    "MLPRegressor 还支持多输出回归，其中样本可以有多个目标。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLPRegressor...\n",
      "真实结果：[1.12  1.072 1.156 0.983 1.168 0.781 0.771 0.923 0.847 0.894]\n",
      "预测结果： [1.14452499 1.11342532 1.02890357 0.79947911 1.02368446 0.56684086\n",
      " 0.84136294 0.77061167 0.79212013 0.8700005 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "\n",
    "X, y = cal_housing.data, cal_housing.target\n",
    "names = cal_housing.feature_names\n",
    "\n",
    "print(\"Training MLPRegressor...\")\n",
    "est = MLPRegressor(activation='logistic')\n",
    "est.fit(X[:-10], y[:-10])\n",
    "print(\"真实结果：{}\".format(y[-10:]))\n",
    "print(\"预测结果：\",est.predict(X[-10:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
