{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>实验3 决策树算法实践 </center>\n",
    "\n",
    "# 一、实验目的\n",
    "\n",
    "通过实验，达到以下目的：\n",
    "\n",
    "- 使学生加深对机器学习过程的理解；\n",
    "- 使学生理解信息熵与信息增益的计算方法；\n",
    "- 使学生掌握决策树的构建方法和程序实现；\n",
    "- 使学生能够熟练应用sklearn库实现基于决策树方法的分类预测。\n",
    "\n",
    "# 二、实验内容\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树（Decision Tree）\n",
    "\n",
    "决策树是一种常见的机器学习方法。常用于分类任务。\n",
    "\n",
    "决策树，顾名思义，是基于树结构来进行决策的，这是人类在面临决策问题时的一种很自然的处理机制。\n",
    "\n",
    "\n",
    "## 基本流程\n",
    "\n",
    "决策树的主要优势在于决策过程很容易理解。利用决策树形成的判断过程，同富有经验的领域专家几乎实现相同的。\n",
    "\n",
    "下面我们分析一下决策树的基本处理流程。\n",
    "\n",
    "以二分类任务为例，我们希望从给定训练数据集学得一个模型，用于对新示例进行分类，这个对样本进行分类的任务，可以看作对“当前样本属于正类么？”这个问题的“决策”或“判定”过程。\n",
    "\n",
    "![决策树示例3](images\\dt\\西瓜问题决策树.png)\n",
    "\n",
    "- 决策过程的最终结论（树的叶子），对应了我们希望的判定结果；\n",
    "\n",
    "- 决策过程中提出的每个判定问题，都是对某个属性的“测试”；\n",
    "\n",
    "- 每个测试的结果，要么导出最终结论，要么导出进一步的判定问题，其考虑范围是在上一次决策结果的限定范围之内；\n",
    "\n",
    "一般地，一颗决策树，包含一个根节点，若干个内部结点，和若干个叶节点。\n",
    "\n",
    "- 叶节点，对应决策结果；\n",
    "\n",
    "- 其它节点，对应一个属性测试。\n",
    "\n",
    "- 每个节点，包含的样本集合，根据属性测试的结果被划分到子节点中；\n",
    "\n",
    "- 根节点，包含样本全集；\n",
    "\n",
    "- 从根节点到每个叶节点的路径，对应了一个判定测试序列。\n",
    "\n",
    "**决策树学习的目的，是为了产生一棵泛化能力强的决策树，基本流程遵循简单而且直观的“分而治之（divide-and-conquer）”策略。**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的构造\n",
    "\n",
    "如何根据训练集数据，建立决策树呢？\n",
    "\n",
    "结合上面的基本过程，我们知道，决策树的每个节点对应了一个属性测试，在数据集中往往有一些属性是“好属性”，使用它们做测试，能够有“正确”的分类结果。\n",
    "\n",
    "\n",
    "### 构造决策树的伪代码\n",
    "\n",
    "输入1：训练集$D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$   \n",
    "输入2：属性集$A = {a_1,a_2,...,a_d}$  \n",
    "过程：函数 TreeGenerate(D,A)\n",
    "\n",
    "\n",
    "生成结点；  \n",
    "\n",
    "if D 中样本全属于同一类别C :  \n",
    "&ensp;&ensp;&ensp;&ensp;将node标记为C类叶结点；  \n",
    "\n",
    "if A = 空集 or D 中样本在 A 上取值相同 :    \n",
    "&ensp;&ensp;&ensp;&ensp; 将 node 标记为叶结点，其类别标记为 D 中样本数最多的类；  \n",
    "&ensp;&ensp;&ensp;&ensp; return；\n",
    "    \n",
    "从 A 中选择最优划分属性 $a_*$;  \n",
    "for $a_*$ 的每一个值 $a_*^v$ :    \n",
    "&ensp;&ensp;&ensp;&ensp;为node生成一个分支；  \n",
    "&ensp;&ensp;&ensp;&ensp;令$D_v$表示D中在$a_*$上取值为$a_*^v$的样本子集；  \n",
    "&ensp;&ensp;&ensp;&ensp;if $D_v$ 为空：  \n",
    "        &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;将分支结点标记为叶节点，其类别标记为D中样本最多的类；  \n",
    "        &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;return    \n",
    "&ensp;&ensp;&ensp;&ensp;else：  \n",
    "        &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;以$TreeGenerate(D_v,A \\ {a_*})$ 为分支结点；  \n",
    "   \n",
    "输出： 以node为根节点的一棵决策树。\n",
    "\n",
    "此时，我们的问题聚焦到了“数据集上哪些特征在划分数据分类时，起决定性作用？”，换句话说，**“如何选择最优划分属性？”**\n",
    "\n",
    "### 划分选择\n",
    "\n",
    "为了找到最好的决策属性，我们需要评估每个属性。那么以何种标准来评估？\n",
    "\n",
    "一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本，尽可能属于同一类别，即结点的“纯度”（purity）越来越高。\n",
    "\n",
    "如果可以度量，那么我们可以根据纯度来评估每个属性，找到最好的决策属性。\n",
    "\n",
    "如何度量样本集合的纯度呢？\n",
    "\n",
    "### 信息熵\n",
    "\n",
    "“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。\n",
    "\n",
    "假定当前样本集合D中第k类样本所占的比例为$p_k(k = 1,2,...,|y|)$，则$D$的信息熵定义为：\n",
    "\n",
    "$Ent(D) = - \\sum_{k=1}^{|y|}p_k \\log_{2}p_k$    ——式（1）\n",
    "\n",
    "Ent(D)的值越小，则D的纯度越高。熵在信息论中代表随机变量不确定度的度量，熵值越大，不确定性越高。\n",
    "\n",
    "> 对熵的直观解释，可以参考https://blog.csdn.net/qq_39521554/article/details/79078917\n",
    "\n",
    "假定离散属性a有V个可能的取值${a^1,a^2,...,a^v}$，若使用a来对样本集D进行划分，则会产生V个分支结点。\n",
    "\n",
    "其中，第$v$个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。\n",
    "\n",
    "根据式（1），计算出$D^v$的信息熵。\n",
    "\n",
    "即当前集合$D^v$，设第k类样本所占的比例为$p_k^v(k = 1,2,...,|y^v|)$\n",
    "\n",
    "$Ent(D^v) = - \\sum_{k=1}^{|y^v|}p_k^v \\log_{2}p_k^v$ \n",
    "\n",
    "\n",
    "下面给出计算$D^v$的香农信息熵的python程序代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据集的香农信息熵：1.584702\n"
     ]
    }
   ],
   "source": [
    "\"\"\"计算香农信息熵的python程序代码\n",
    "\"\"\"\n",
    "from math import log\n",
    "from sklearn import neighbors,datasets\n",
    "import pandas as pd\n",
    "\n",
    "def calcShannonEnt(dataFrame):\n",
    "    \"\"\"\n",
    "    功能：根据分类标记，计算某数据集的信息熵。\n",
    "    输入：dataFrame，使用pandas.seriers类型给出的含有标记的数据集，标记信息为最后一列\n",
    "    输出：shannonEnt，数据集按当前标记分类结果的信息熵值\n",
    "    \n",
    "    \"\"\"  \n",
    "    numEntries = dataFrame.shape[0] #s数据集示例数\n",
    "    \n",
    "    labelCounts = {} #定义字典，键为分类标记名，值为标记的计数值\n",
    "    labelCounts.update(dataFrame.iloc[:,-1].value_counts())#为字典赋值,认为数据集最后一列为label\n",
    "    \n",
    "    shannonEnt = 0.0  # 设置香农信息熵初值为0.0\n",
    "    for key in labelCounts:\n",
    "        # 按公式求信息熵值\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        \n",
    "        shannonEnt -= prob * log(prob,2)    # 求以2为底的对数。\n",
    "    return shannonEnt\n",
    "\n",
    "def calcDatingDataSetEnt():\n",
    "    dataframe = pd.read_csv(\"data\\dating\\datingTestSet.txt\",header=None,sep='\\t',names=['年飞机里程','周冰淇淋升数','游戏耗时比','心仪程度'])\n",
    "    entropy = calcShannonEnt(dataframe)\n",
    "    print(\"当前数据集的香农信息熵：%f\"  % entropy)\n",
    "\n",
    "calcDatingDataSetEnt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子中计算得到的信息熵差别不大，我们再看一个例子。\n",
    "\n",
    "这个例子是美国大选投票数据集，基本情况如下图所示：\n",
    "\n",
    "![选举数据示例](images/dt/voteexample.png)\n",
    "\n",
    "这个例子将计算一个投票数据集的熵值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息熵：0.996566\n"
     ]
    }
   ],
   "source": [
    "def calcAllVoteFeaturesEnt():\n",
    "    dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "    \n",
    "    entropy = calcShannonEnt(dataframe)\n",
    "    print(\"信息熵：%f\"  % entropy)\n",
    "\n",
    "calcAllVoteFeaturesEnt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益\n",
    "\n",
    "假定离散属性a有V个可能的取值${a^1,a^2,...,a^v}$，若使用a对样本集D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。\n",
    "\n",
    "我们可以根据式（1）计算出$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$\\frac{|D^v|}{|D|}$，即样本数越多的分支终点的影响越大，于是可计算出用属性a对样本集D进行划分所获得的“信息增益”（information gain)。\n",
    "\n",
    "$Gain(D,a) = Ent(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}Ent(D^v)$  ——式（2）\n",
    "\n",
    "一般而言，信息增益越大，则意味着使用属性a来进行划分所获得的“纯度提升”越大。\n",
    "\n",
    "因此，我们可用信息增益来进行决策树的划分属性选择，即在选择“最好的”属性进行决策。\n",
    "\n",
    "$a_* = argmax_{a \\in A}Gain(D,a)$\n",
    "\n",
    "著名的ID3决策树学习算法【Quinlan，1986】就是以信息增益为准则来划分属性。\n",
    "\n",
    "下面程序的例子是以美国大选的投票数据样本集为例，选择最好划分属性的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('纹理', 0.3805918973682686)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcInforGain(df,aFeature):\n",
    "    \"\"\"\n",
    "    功能：按照上述公式计算用属性aFeature对样本集dataframe进行划分的信息增益。\n",
    "    输入：数据集dataFrame；\n",
    "          划分属性名aFeature；\n",
    "    输出：（最好划分属性名称，最大信息增益值）\n",
    "    \"\"\"\n",
    "    # 统计数据集的样本数量\n",
    "    totalsampleCount = df.shape[0]\n",
    "    # 统计属性a各值对应的样本数量\n",
    "    sampleCounts = {} \n",
    "    sampleCounts.update(df[aFeature].value_counts())\n",
    "    #print(sampleCounts)\n",
    "    infogain = calcShannonEnt(df)\n",
    "    for key,value in sampleCounts.items():\n",
    "        subdf = df[df[aFeature] == key]\n",
    "        infogain -= subdf.shape[0]/ totalsampleCount * calcShannonEnt(subdf)\n",
    "    return infogain\n",
    "\n",
    "def getBestDivideFeature(df):    \n",
    "    featureInfoGains = {}    \n",
    "    for colname in df.columns[:-1]:\n",
    "        # 对非标记属性，计算其信息增益，标记属性为dataframe中最后一列\n",
    "        infogain = calcInforGain(df,colname)\n",
    "        featureInfoGains[colname] = infogain\n",
    "    # 对已计算增益的结果进行排序\n",
    "    bestFeature = sorted(featureInfoGains.items(),key =lambda item:item[1],reverse=True)[0]\n",
    "    #print(featureInfoGains)\n",
    "    return bestFeature\n",
    "\n",
    "#dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')        \n",
    "#getBestDivideFeature(dataframe)    \n",
    "dataframe = pd.read_csv(\"data/maloon/maloon2.txt\",header=0,sep=',')        \n",
    "getBestDivideFeature(dataframe.iloc[:,1:])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据计算信息增益，发现在投票数据集中，“医师费用冻结”属性对数据集进行划分的信息增益最大，于是我们将选择它作为划分属性。\n",
    "\n",
    "划分结果如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "按\"医师费用冻结\"进行划分:\n",
      "    n类的节点数为119\n",
      "    y类的节点数为113\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "dict = {}\n",
    "dict.update(dataframe[\"医师费用冻结\"].value_counts())\n",
    "subDList = {}\n",
    "for colValue in dict.keys():\n",
    "    subDList[colValue] = dataframe[dataframe[\"医师费用冻结\"]==colValue]\n",
    "\n",
    "print(\"按\\\"医师费用冻结\\\"进行划分:\")\n",
    "for i in subDList.items():\n",
    "    print(\"    {}类的节点数为{}\".format(i[0],i[1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，决策树学习算法将对每个分支结点做进一步划分。\n",
    "\n",
    "如上例中，通过“医师费用冻结”进行划分后，需要再次计算“最好划分属性”，对各分支结点所含数据集进行划分。\n",
    "\n",
    "### 构建决策树的程序\n",
    "\n",
    "选择最佳属性的过程一般需要多次，对每个分支结点进行上述操作，直至将所有属性都使用完毕。这样一棵决策树就建立起来了。\n",
    "\n",
    "但是，大多数用户不需要这样的决策树，而是需要那种只通过3~5个属性值就能够进行分类的决策树。\n",
    "\n",
    "这时需要由用户事先指定“树的深度”这个超参来构建决策树。\n",
    "\n",
    "下面，根据上文中构造决策树的伪代码，我们编写决策树构建函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Node:\n",
    "    \"\"\"决策树结点类\"\"\"\n",
    "    def __init__(self):\n",
    "        self._type = None\n",
    "        self._label = None\n",
    "        self._samples = pd.DataFrame()\n",
    "        self._children = {}        \n",
    "    \n",
    "    @property    \n",
    "    def type(self):\n",
    "        return self._type\n",
    "    \n",
    "    @type.setter\n",
    "    def setType(self,type):\n",
    "        self._type = type\n",
    "    \n",
    "    @property    \n",
    "    def label(self):\n",
    "        return self._label\n",
    "    \n",
    "    @label.setter\n",
    "    def setLabel(self,label):\n",
    "        self._label = label\n",
    "        \n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples\n",
    "    \n",
    "    @samples.setter\n",
    "    def setSamples(self,samples):\n",
    "        if isinstance(samples, pd.DataFrame):\n",
    "            self._samples = samples\n",
    "    \n",
    "    @property    \n",
    "    def children(self):\n",
    "        return self._children\n",
    "    \n",
    "\n",
    "    def addChildren(self,key,children):\n",
    "        self._children[key] = children\n",
    "\n",
    "    def getChildrenTypeList(self):\n",
    "        cl = []\n",
    "        for k,v in self._children.items():\n",
    "            cl.append(v.type + '['+k+']')\n",
    "        return cl\n",
    "    \n",
    "    def __str__(self):        \n",
    "        return \"Type:{},Label:{},Samples:{},Children:{} \".format(\n",
    "                self._type,self._label,\n",
    "                self._samples.index.tolist(),\n",
    "                self.getChildrenTypeList())\n",
    "def bfs(rootNode,depth = 0):\n",
    "    print(\"{}{}\".format('\\t'*depth,rootNode))\n",
    "    #print(\"Type:{},Label:{},Samples:{}\".format(rootNode.type,rootNode.label,rootNode.samples.index.tolist()))\n",
    "    for k,v in rootNode.children.items():\n",
    "        bfs(v,depth + 1)\n",
    "    return depth\n",
    "        \n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的代码用于测试上述类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:root,Label:None,Samples:[],Children:['middle[a=1]', 'middle[a=2]'] \n",
      "Type:root,Label:None,Samples:[],Children:['middle[a=1]', 'middle[a=2]'] \n",
      "\tType:middle,Label:None,Samples:[],Children:['leaf[b=1]', 'leaf[b=2]'] \n",
      "\t\tType:leaf,Label:None,Samples:[],Children:[] \n",
      "\t\tType:leaf,Label:None,Samples:[],Children:[] \n",
      "\tType:middle,Label:None,Samples:[],Children:[] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = Node()\n",
    "node.setType = 'root' \n",
    "child = Node()\n",
    "node.addChildren('a=1',child)\n",
    "child.setType = 'middle'\n",
    "child2 = Node()\n",
    "node.addChildren('a=2',child2)\n",
    "child2.setType = 'middle'\n",
    "ss = Node()\n",
    "ss.setType = 'leaf'\n",
    "ss1 = Node()\n",
    "ss1.setType = 'leaf'\n",
    "child.addChildren('b=1',ss)\n",
    "child.addChildren('b=2',ss1 )\n",
    "print(node)\n",
    "bfs(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def createDT(dataFrame,depth):\n",
    "    \"\"\"\n",
    "    功能：根据有标记数据集dataFrame，构建深度为depth的决策树\n",
    "    输入：训练集 𝐷 = dataFrame ，属性名为dataframe的第0行\n",
    "          树的深度为depth，默认值为3\n",
    "    输出：一棵以嵌套字典表示的决策树  \n",
    "\n",
    "    \"\"\"\n",
    "    # 生成结点\n",
    "    node = Node()\n",
    "         \n",
    "    if len(dataFrame.iloc[:,-1].value_counts()) == 1:\n",
    "        #若D 中样本全属于同一类别，则将node标记为C类叶结点\n",
    "        node.setType = 'Leaf'\n",
    "        node.setLabel = dataFrame.iloc[0,-1]\n",
    "        node.setSamples = dataFrame    \n",
    "        return node\n",
    "\n",
    "    if len(dataFrame.iloc[:,:-1]) == 0 :\n",
    "        # A = 空集 :\n",
    "        # 将 node 标记为叶结点，其类别标记为 D 中样本数最多的类；\n",
    "        # 注意，因为可能存在未知因素，会存在A上取值相同，但分类不同的示例。        \n",
    "        tempdict = {}\n",
    "        tempdict.update(dataFrame.iloc[:,-1].value_counts())  \n",
    "        if tempdict:\n",
    "            label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]  \n",
    "        else:\n",
    "            label = ''\n",
    "        node.setType = 'Leaf' \n",
    "        node.setLabel = label\n",
    "        node.setSamples = dataFrame\n",
    "        return node\n",
    "    \n",
    "    valueIsUnique = False\n",
    "   \n",
    "    for feature in dataFrame.columns[:-1]:    \n",
    "        if len(dataFrame[feature].value_counts()) > 1:\n",
    "            # 属性集任一个属性的取值不唯一，就跳出\n",
    "            break\n",
    "        valueIsUnique = True\n",
    "    if valueIsUnique :\n",
    "        # D 中样本在 A 上取值相同 :\n",
    "        # 将 node 标记为叶结点，其类别标记为 D 中样本数最多的类；\n",
    "        # 注意，因为可能存在未知因素，会存在A上取值相同，但分类不同的示例。        \n",
    "        tempdict = {}\n",
    "        tempdict.update(dataFrame.iloc[:,-1].value_counts())        \n",
    "        label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]        \n",
    "        node.setType = 'Leaf' \n",
    "        node.setLabel = label\n",
    "        node.setSamples = dataFrame      \n",
    "        return node\n",
    "  \n",
    "    #从 A 中选择最优划分属性  𝑎∗ \n",
    "    \n",
    "    aFeature = getBestDivideFeature(dataFrame)[0]    \n",
    "    #print(\"  使用{}作为划分依据\".format(aFeature))\n",
    "    node.setType = \"据\\\"{}\\\"划分\".format(aFeature)\n",
    "    node.setLabel = '未定'\n",
    "    node.setSamples = dataFrame\n",
    "    \n",
    "    dict = {}\n",
    "    dict.update(dataFrame[aFeature].value_counts())\n",
    "    #print(\"dict内容：{}\".format(dict))\n",
    "    for colValue in dict.keys():\n",
    "        keyname = aFeature+'='+colValue\n",
    "        #对𝑎∗ 的每一个值  𝑎𝑣∗ ，为node生成一个分支 \n",
    "        aBranchNode = Node()                      \n",
    "        #令 𝐷𝑣 表示D中在 𝑎∗ 上取值为 𝑎𝑣∗ 的样本子集；\n",
    "        dv = dataFrame[dataFrame[aFeature]==colValue]         \n",
    "        if  dv.empty:\n",
    "            # 若dv为空,将分支结点标记为叶节点，其类别标记为D中样本最多的类；\n",
    "            tempdict = {}\n",
    "            tempdict.update(dataFrame.iloc[:,-1].value_counts())        \n",
    "            label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]              \n",
    "            aBranchNode.setType = 'Leaf' \n",
    "            aBranchNode.setLabel = label\n",
    "            aBranchNode.setSamples = dv\n",
    "            # 将aBranchNode列为node的子节点           \n",
    "            node.addChildren(keyname,aBranchNode)              \n",
    "        else:    \n",
    "            # 去除数据集Dv 中aFeature列后的样本\n",
    "            subDv = dv.drop([aFeature],axis =1)            \n",
    "            if depth == 0:\n",
    "                #若当前树的深度已经达到要求，则将当前分支节点的其类别标记为D中样本最多的类\n",
    "                tempdict = {}\n",
    "                tempdict.update(subDv.iloc[:,-1].value_counts())        \n",
    "                label = sorted(tempdict,key =lambda item:item[1],reverse=True)[0]              \n",
    "                aBranchNode.setType = 'Leaf' \n",
    "                aBranchNode.setLabel = label    \n",
    "                aBranchNode.setSamples = subDv\n",
    "                # 将aBranchNode列为node的子节点                \n",
    "                node.addChildren(keyname,aBranchNode)     \n",
    "                continue\n",
    "            # 以 createDT(subDv)  为分支结点；\n",
    "            node.addChildren(keyname,createDT(subDv,depth-1))\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们以西瓜数据集进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataframe = pd.read_csv(\"data/vote/votesimple.txt\",header=0,sep=',')    \n",
    "df = pd.read_csv(\"data1/maloon/maloon2.txt\",header=0,sep=',')\n",
    "d = df.iloc[:,1:]\n",
    "dtree = createDT(d,depth = 5)\n",
    "print('决策树：')\n",
    "bfs(dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn库决策树分类器应用\n",
    "\n",
    "上面的程序是构建决策树的示例。在实际应用中，我们往往使用较为成熟的第三方工具sklearn。\n",
    "\n",
    "下面的示例中，使用sklearn.tree 中的DecisionTreeClassifier对投票数据进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "democrat\n",
      "democrat\n",
      "democrat\n",
      "democrat\n",
      "democrat\n",
      "republican\n",
      "republican\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def decisionTree(list):\n",
    "    dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "    #data.describe()\n",
    "    X = dataframe.iloc[:,:-1] #存放训练样本中无标记的数据\n",
    "    X1 = pd.DataFrame() # sklearn的算法分类器大多只处理数值型矩阵，X1将存放数值化的样本\n",
    "    for column in X.columns:\n",
    "        X1[column] = X[column].apply(lambda x:1 if x=='y' else 2)\n",
    "        \n",
    "    y = dataframe.iloc[:,-1] # 存放标记    \n",
    "    y1 = y.apply(lambda x:1 if x == \"republican\" else 2) # 对标记进行数值化\n",
    "        \n",
    "    # 使用决策树模型对X,y进行拟合，即生成决策树分类器\n",
    "    clf = DecisionTreeClassifier(max_depth=3)\n",
    "    clf.fit(X1, y1)\n",
    "    # 对输入的list按上面生成的决策树分类器进行批量预测\n",
    "    predict = clf.predict([list])     \n",
    "    return predict[0]\n",
    "\n",
    "def predict():\n",
    "    dataframe = pd.read_csv(\"data/vote/Vote.csv\",header=None,sep=',')    \n",
    "    for index,row in dataframe.iterrows():\n",
    "        #print(row.tolist()[:-1])\n",
    "        row = row.apply(lambda x:1 if x=='y' else 2)\n",
    "        result = decisionTree(row.tolist()[:-1])\n",
    "        if result == 1:\n",
    "            print(\"republican\")\n",
    "        else:\n",
    "            print(\"democrat\")\n",
    "        \n",
    "predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解sklearn的决策树分类器\n",
    "\n",
    "下面我们通过分析程序来理解上述决策树的基本情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes=11\n",
      "children_left=[ 1  2  3 -1 -1 -1  7 -1  9 -1 -1]\n",
      "children_right=[ 6  5  4 -1 -1 -1  8 -1 10 -1 -1]\n",
      "feature=[ 3 10  8 -2 -2 -2  2 -2  5 -2 -2]\n",
      "threshold=[ 1.5  1.5  1.5 -2.  -2.  -2.   1.5 -2.   1.5 -2.  -2. ]\n",
      "The binary tree structure has 11 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 3] <= 1.5 else to node 6.\n",
      "\tnode=1 test node: go to node 2 if X[:, 10] <= 1.5 else to node 5.\n",
      "\t\tnode=2 test node: go to node 3 if X[:, 8] <= 1.5 else to node 4.\n",
      "\t\t\tnode=3 leaf node.\n",
      "\t\t\tnode=4 leaf node.\n",
      "\t\tnode=5 leaf node.\n",
      "\tnode=6 test node: go to node 7 if X[:, 2] <= 1.5 else to node 8.\n",
      "\t\tnode=7 leaf node.\n",
      "\t\tnode=8 test node: go to node 9 if X[:, 5] <= 1.5 else to node 10.\n",
      "\t\t\tnode=9 leaf node.\n",
      "\t\t\tnode=10 leaf node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataframe = pd.read_csv(\"data/vote/VoteTraining-cn.csv\",header=0,sep=',')\n",
    "#data.describe()\n",
    "X = dataframe.iloc[:,:-1] #存放训练样本中无标记的数据\n",
    "X1 = pd.DataFrame() # sklearn的算法分类器大多只处理数值型矩阵，X1将存放数值化的样本\n",
    "for column in X.columns:\n",
    "    X1[column] = X[column].apply(lambda x:1 if x=='y' else 2)\n",
    "y = dataframe.iloc[:,-1] # 存放标记    \n",
    "y1 = y.apply(lambda x:1 if x == \"republican\" else 2) # 对标记进行数值化\n",
    "\n",
    "# 使用决策树模型对X,y进行拟合，即生成决策树分类器\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X1, y1)\n",
    "\n",
    "n_nodes = clf.tree_.node_count\n",
    "children_left = clf.tree_.children_left\n",
    "children_right = clf.tree_.children_right\n",
    "feature = clf.tree_.feature\n",
    "threshold = clf.tree_.threshold\n",
    "    \n",
    "print(\"n_nodes={}\".format(n_nodes))\n",
    "print(\"children_left={}\".format(children_left))\n",
    "print(\"children_right={}\".format(children_right))\n",
    "print(\"feature={}\".format(feature))\n",
    "print(\"threshold={}\".format(threshold))\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、实验要求\n",
    "\n",
    "1. 学生应当能够在教师的指导下理解上述处理过程和python代码实现；\n",
    "2. 参考实验内容完成一个自选新数据集的分类预测；\n",
    "3. 根据实验过程与结果编写实验报告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "312.77px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
